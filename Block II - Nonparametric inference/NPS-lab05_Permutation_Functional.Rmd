---
title: "Lab 05 - Permutation tests for Functional Data"
date: 2024-10-25
author: "Nonparametric statistics AY 2024-2025"
output:
  html_document: 
    df_print: paged
    toc: true
  pdf_document: default
  word_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(rgl)
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_hooks$set(webgl = hook_webgl)
```

```{css, echo=FALSE}
.extracode {
background-color: lightblue;
}
```

# Global Testing

The Objective of this lab is to show you some applications of the advanced techniques you have seen during the course, applied to functional data. Namely we will see permutation testing (both global and local...).

Let's load the packages we need and our data:

```{r}
library(fda)
library(roahd)

data=growth #data from the berkeley growth study...
```

And let's plot the curves...

```{r}
matplot(data$age,data$hgtm, type='l',col='blue',ylab="height",
        main="Berkley Growth data set")
matlines(data$age,data$hgtf, type='l',col='red')
```


What if I want to test if the two curves are equal or not? Nothing simpler.. I just need to remember how permutation tests work...

## Permutation global test for equality of functional means

```{r}
seed=20
B=1e3

berkeley=rbind(t(data$hgtm),t(data$hgtf))
n=nrow(berkeley)
n_m=nrow(t(data$hgtm))
n_f=nrow(t(data$hgtf))


meandiff=(colMeans(t(data$hgtm))-colMeans(t(data$hgtf)))
plot(meandiff,type = 'l', main="Mean difference function")
T0=sum(meandiff^2)
T0
```

And, Knowing that under $H_0$ the two groups of curves are i.i.d, my likelihood-invariant permutation scheme is of course label permutation, so...

```{r}
T0_perm=numeric(B)

for(perm in 1:B){
  permutation <- sample(n)
  berkeley_perm=berkeley[permutation,]
  perm_m = berkeley_perm[1:n_m,] 
  perm_f = berkeley_perm[(n_m+1):n,] 
  T0_perm[perm]=sum(((colMeans(perm_m)-colMeans(perm_f)))^2)
}

sum(T0_perm >= T0)/B
hist(T0_perm,xlim = c(0,2000))
abline(v=T0,col='green')

```

What would have happened instead, if I were to test inside a group?

```{r}
male1=berkeley[1:(n_m/2),]
male2=berkeley[(n_m/2):n_m,]
ber_m=rbind(male1,male2)

T0=sum(((colMeans(male1)-colMeans(male2)))^2)
T0

T0_perm=numeric(B)

for(perm in 1:B){
  permutation <- sample(n_m)
  berkeley_perm=ber_m[permutation,]
  perm_m = berkeley_perm[1:(n_m/2),] 
  perm_f = berkeley_perm[(n_m/2):n_m,] 
  T0_perm[perm]=sum(((colMeans(perm_m)-colMeans(perm_f)))^2)
}

sum(T0_perm >= T0)/B
hist(T0_perm)
abline(v=T0,col='green')
```

Expectedly, I am not rejecting the null hypothesis (Pvalue of the test is very high...).

## Permutation global test for equality of functional medians


Of course, I can think about using different test statistics.

To do so, though, I will need a slightly different technique to treat functional data, using the package `roahd`.


```{r}
hgtm_fd=fData(data$age,t(data$hgtm))
hgtf_fd=fData(data$age,t(data$hgtf))

meandiff=median_fData(hgtm_fd,type='MBD')-median_fData(hgtf_fd,type='MBD')
plot(meandiff,type = 'l')
T0=(sum(abs(meandiff$values)))
T0
```
And now, the test

```{r}
berkeley_fd=append_fData(hgtm_fd,hgtf_fd)

for(perm in 1:B){
  permutation <- sample(n)
  berkeley_perm=berkeley_fd[permutation,]
  perm_m = berkeley_perm[1:n_m,] 
  perm_f = berkeley_perm[(n_m+1):n,] 
  meandiff=median_fData(perm_m,type='MBD')-median_fData(perm_f,type='MBD')
  T0_perm[perm]=sum(abs(meandiff$values))
}

sum(T0_perm >= T0)/B
hist(T0_perm,xlim = c(0,300))
abline(v=T0,col='green')
```


## Example: last year's January exam

Before having a fallout with his (former) friend Dr. Marius, Dr. Sulla received from him a data set of smoothed functions for the observations `CO2_emissions` along time from $1990$ to $2010$, in an equally spaced grid of $100$ elements and only for `High income` and `Low income` countries. `ex3.rds` contains a list with the data frame and the time grid.

1. By suitably defining a functional data object, plot the resulting curves for the $N = 57$ countries. Build a functional boxplot without adjusting the F value and report which countries are magnitude outliers.

```{r}
library(roahd)
data3 <- readRDS(file="./data/ex3.rds")
df.func<- data3$df3
time <- data3$time
f_data <- fData(time, df.func[, -c(1,2)])
plot(f_data) # what happens if I do plot(data)?

```


```{r}
fbp <- fbplot(f_data, main="Magnitude outliers",adjust = F)
df.func[fbp$ID_outliers,c(1,2)]

```



3. Following the logic of the EKC, high and low income countries should produce different `C02_emissions`.To see whether this is verified in the C02_emissions measured along time, perform a global permutation test for the equality of both functional medians of both groups, using the $L2$ distance between them.
You can approximate it with the euclidean distance of the discretely sampled curves.


```{r}
euclidean <- function(a, b) sqrt(sum((a - b)^2))
mean.high <- apply(df.func[df.func$IncomeGroup=="High income", -c(1,2)], MARGIN=2, FUN=mean)
mean.low <- apply(df.func[df.func$IncomeGroup=="Low income", -c(1,2)], MARGIN=2, FUN=mean)

T0 <- euclidean(mean.high, mean.low)
n <- dim(df.func)[1]
B <- 1000
Tperm <- numeric(B)
set.seed(2024)
for (b in 1:B){
  perm <- sample(1:n, replace = F)
  dfperm <- df.func
  dfperm[, -c(1,2)] <- df.func[perm, -c(1,2)]
  mean.high.perm <- apply(dfperm[dfperm$IncomeGroup=="High income", -c(1,2)], MARGIN=2, FUN=function(x){mean(x)})
  mean.low.perm <- apply(dfperm[dfperm$IncomeGroup=="Low income", -c(1,2)], MARGIN=2, FUN=function(x){mean(x)})
  Tperm[b] <- euclidean(mean.high.perm, mean.low.perm)
}
sum(Tperm>=T0)/B
```


* Bonus: what if we wanted a better approximation of the $L2$ distance?

```{r}
L2.distance <- function(f1, f2, grid){
  
  return( (pracma::trapz(x=grid, y=(f1-f2)**2) )**(1/2) )
}

mean.high <- apply(df.func[df.func$IncomeGroup=="High income", -c(1,2)], MARGIN=2, FUN=mean)
mean.low <- apply(df.func[df.func$IncomeGroup=="Low income", -c(1,2)], MARGIN=2, FUN=mean)

T0 <- L2.distance(f1=mean.high, f2=mean.low, grid=time)
n <- dim(df.func)[1]
B <- 1000
Tperm <- numeric(B)
set.seed(2024)
for (b in 1:B){
  perm <- sample(1:n, replace = F)
  dfperm <- df.func
  dfperm[, -c(1,2)] <- df.func[perm, -c(1,2)]
  mean.high.perm <- apply(dfperm[dfperm$IncomeGroup=="High income", -c(1,2)], MARGIN=2, FUN=function(x){mean(x)})
  mean.low.perm <- apply(dfperm[dfperm$IncomeGroup=="Low income", -c(1,2)], MARGIN=2, FUN=function(x){mean(x)})
  Tperm[b] <-  L2.distance(f1=mean.high.perm, f2=mean.low.perm, grid=time)
}

hist(Tperm, xlim=c(0,50))
abline(v=T0)
sum(Tperm>=T0)/B
```

## Independence test

Let us do an independence test between two functional data populations.

We use the ECG data from `Lab02`.

```{r}

bivar.func.data <- as.mfData(list(mfD_healthy$fDList[[1]], mfD_healthy$fDList[[2]]))
plot(bivar.func.data)

```

If you recall the first lectures, we can utilise Spearman's correlation coefficient for functional data: we compute ranks with the `Modified Hypograph Index`, then a correlation coefficient, yielding a test statistic.

The permutational scheme is the same as the one we saw yesterday.

```{r}
MHI.first.func.dim <- MHI(bivar.func.data$fDList[[1]]) # modified hypograph index
MHI.second.func.dim <- MHI(bivar.func.data$fDList[[2]])

T0 <- cor_spearman(bivar.func.data, ordering='MHI')
T0 <- cor(MHI.first.func.dim, MHI.second.func.dim)

n <- bivar.func.data$fDList[[1]]$N
# n <- bivar.func.data$fDList[[2]]$N

T0.perm.distrib <- numeric(B)

for(perm in 1:B){
  permutation.x <- sample(n, replace=F)
  permutaiton.y <- sample(n, replace=F)
  T0.perm.distrib[perm] <- cor(MHI.first.func.dim[permutation.x], 
                               as.numeric(MHI.second.func.dim)[permutaiton.y])**2
  # alternative
  #T0.perm.distrib[perm]  <- cor_spearman(as.mfData(list(mfD_healthy$fDList[[1]][permutation.x], 
    #                          mfD_healthy$fDList[[2]][permutaiton.y])
      #                   ), 
      #         ordering="MHI")**2
  
  
  }

hist(T0.perm.distrib, xlim=c(0,.7))
abline(v=T0)
sum(T0.perm.distrib >=T0)/B


```

#### Exercise: implement an independence test for two multivariate populations.


## An exercise: Functional Mann-Whitney

In order to explore different tests, let us exploit further depth measures.

Recall the Mann-Whitney U test: we have 
$$
H_0: X \stackrel{d}{=} Y \,\,\, vs. \,\,\, H_1: P[X>Y] \neq 0.5
$$
and we look at possible deviations towards $H_1$ using the ranks of data.

**BUT** we have just computed ranks for Spearman's correlation coefficient for functional data...

So all we need is a function that ranks our functional data.


```{r}
rank.func.data <- function(fdata){
   return (
     rank( MHI(fdata) ) 
           )

}
```
So we are done! We can paste Prof Vantini's code...

We use the Berkley data set in this example.

```{r,eval=T}
func.data.bk <- fData(growth$age, t(cbind(growth$hgtm, growth$hgtf)) )
n1 <- dim(growth$hgtm)[2]
n2 <- dim(growth$hgtf)[2] 
male.indices <- 1:n1
female.indices <- 1:n2 + max(male.indices)
plot(func.data.bk)
```

```{r}

R <- rank.func.data(func.data.bk)
R1 <- sum(R[male.indices])
R2 <- sum(R[female.indices])

U1 <- R1 - n1*(n1+1)/2  # Nr of wins of the 1st sample
U2 <- R2 - n2*(n2+1)/2  # Nr of wins of the 2nd sample


set.seed(24021979)
B <- 100000
U1.sim <- numeric(B)
U2.sim <- numeric(B)
n <- n1+n2
for (k in 1:B)
{
  ranks.temp <- sample(1:n)
  R1.temp <- sum(ranks.temp[1:n1])
  R2.temp <- sum(ranks.temp[(n1+1):(n1+n2)])
  U1.temp <- R1.temp - n1*(n1+1)/2
  U2.temp <- R2.temp - n2*(n2+1)/2
  U1.sim[k] <- U1.temp
  U2.sim[k] <- U2.temp
}

hist(U1.sim, breaks = 50)
abline(v = c(U1, U2), col='red')
abline(v = n1*n2/2, lwd=3)

hist(U2.sim, breaks = 50)
abline(v = c(U1, U2), col='red')
abline(v = n1*n2/2, lwd=3)

U.star <- max(U1, U2)
abline(v=U.star)

p.value <- 2 * sum(U1.sim >= U.star)/B
p.value

```



# Local testing


Now, what I am doing here is basically is testing the hypothesis globally, I am rejecting if, for at least one time instant $t$ the two curves are statistically different. How do I tell what is that specific time instant? I use a procedure called Interval-wise Testing.

## A first look: January's exam

2. Provide the (point-wise) mean of each group. Add them the last plot. Calculate the pointwise p-value of the permutation test using as test statistic the (absolute) difference of the means and interpret your results.

We firstly recover the plot
```{r}
plot(f_data[-fbp$ID_outliers]) 
mean.high <- apply(df.func[df.func$IncomeGroup=="High income", -c(1,2)], MARGIN=2, FUN=function(x){mean(x)})
lines(time, mean.high,col="black")
mean.low <- apply(df.func[df.func$IncomeGroup=="Low income", -c(1,2)], MARGIN=2, FUN=function(x){mean(x)})
lines(time, mean.low,col="black")
```

And now perform the point-wise test. This means we have a family of hypothesis tests:

$$
H_{0,s} = \mathbb{E}[X(s)] = \mu(s)\; vs. \; H_{1,s} : \mathbb{E}[X(s)] \neq \mu(s); \; s \in \mathcal{S}
$$
where again $X(s)$
By fixing $s$, we have a typical univariate permutation test for a hypothesised mean. So let us compute with the same loop all the different p-values of the different tests (one for each $s \in \mathcal{S}$), in an analogos way to what we did with the Bootstrap.
We take 
$$\mu(s) = 0.63, \forall s \in \mathcal{S}$$
And compute the point-wise test statistic:
$$
T(s) = |\hat{\mu}(s) - \mu(s) |\; \forall s \in \mathcal{S}
$$
where $\hat{\mu}(s)$ is the sample mean at $s$.
```{r,eval=F}
mu.H0 <- 0.063
T0s <- abs(apply(func.data.sample, MARGIN=2, FUN=mean) - mu.H0)
```
And now we estimate the permutational distribution conditional on sample `func.data.sample` at point $s$

1. Estimate permutational distribution of the test statistic at each $s$
2. Compute p-value of every test $s \in \mathcal{S}$

```{r}

uniperm=function(tindex,B=100){
  data <- df.func[, c(2,tindex+2)]
  n <- dim(data)[1]
  Tperm <- numeric(B)
  mean.high.pw <- mean(data[data$IncomeGroup=="High income", -1])  
  mean.low.pw <- mean(data[data$IncomeGroup=="Low income", -1])
  t0 <- sqrt((mean.high.pw- mean.low.pw)**2)

  for(index in 1:B){
    perm <- sample(1:n, replace = F)
    dfperm <- data
    dfperm[, -1] <- data[perm, -1]
    mean.high.perm <- mean(dfperm[dfperm$IncomeGroup=="High income", -1])
    mean.low.perm <- mean(dfperm[dfperm$IncomeGroup=="Low income", -1])
  Tperm[index] <- sqrt((mean.high.perm - mean.low.perm)**2)
  }
  return(sum(Tperm>=t0)/B)
}

pval.fun=numeric(length(time))
for(index in 1:length(time)){
  set.seed(2024)
  pval.fun[index]=uniperm(index)
}

plot(time, pval.fun, ylim=c(0,1))
```


### Permutation tests for functional data: unadjusted p-value function

**Details on the implementation**.

As all FDA techniques, the procedure to
evaluate the unadjusted and adjusted p-value functions and select the significant intervals of the domain described in Section 2 has to be numerically approximated to deal with the analysis of real data.

* _Step 1_ : **smoothing**.

* _Step 2_: pointwise p-value function

* _Step 3_ interval-wise p-value function


Coherently with previous analyses of the Canadian daily temperature dataset presented in the literature (Ramsay and Silverman 2005), the functional data have been obtained by means of a Fourier smoothing on 65 harmonics. The number of harmonics was selected through a cross-validation procedure.
The test statistic (3) is evaluated by means of a numerical integration method,
through a trapezoidal rule based on a fine uniform grid of 365 knots. The same
evaluation is used to perform the test proposed by Vsevolozhskaya et al. (2014).
Since the number of permutations of the data to be explored to evaluate the
p-values is extremely high in at least some of the studied cases, a Conditional
Monte Carlo (CMC) algorithm is applied to estimate the p-values of the tests
of HI
0
against HI
1
for any interval I (Pesarin and Salmaso 2010). In the case of
the analysis reported in this section a CMC algorithm based on 1000 randomly
chosen permutations was employed. Note that, to decrease the computational
cost of the permutation method applied to any interval I, it is possible to perform
all tests based on the same set of permutations.
Finally, the p-values of the tests HI
0
against HI
1
for any interval I are discretized on a sufficiently fine grid. Relying on the continuity of the test statistic
(3) with respect to the extremes of the integration interval, the max and lim sup
of Definition 2.1 have been approximated with their discrete counterparts.




```{r,eval=F}
T.s.B = matrix(nrow=B, ncol=length(grid)) 
n <- dim(func.data.sample)[1]
p <- dim(func.data.sample)[2]

set.seed(SEED)
for(b in 1:B){

  # Permuted dataset (reflection-based)
  signs.perm = rbinom(n, 1, 0.5)*2 - 1
  
  func.perm = mu.H0 + (func.data.sample - mu.H0) * matrix(signs.perm, nrow=n,ncol=p,byrow=FALSE)
  
  
  T.s.B[b, ]  = apply(func.perm, MARGIN=2, FUN=function(x) abs(mean(x)-mu.H0) )
}
# compute element-wise p-values
p.vals <- sapply(1:p, function(i) sum(T.s.B[,i]>=T0s[i])/B )

```
So let us plot firstly the functional dataset with the hypothesised mean:
```{r,eval=F}
matplot(t(func.data.sample), type="l", col="black")
lines(m1, col="red")
lines(apply(func.data.sample, MARGIN=2, FUN=mean), col="turquoise")
lines(rep(mu.H0, P), col="green")
```

And now the **unadjasted** p-value function:
```{r,eval=F}
plot(x=grid, y=p.vals, type="l", main="Unadjasted p-value function")
```




Let's load the package

```{r}
#devtools::install_github("alessiapini/fdatest")
library(fdatest)
```

And let's run the test: I will show just a 2 sample case, very simple and straightforward...

```{r}

tst=IWT2(t(data$hgtm),t(data$hgtf))
plot(tst)

```

This technique allows you to perform a two sample t-test AND to impute a rejection of the null to some parts of the domain. (shadings represent significance values, dark grey is 1%, light is 5%).
The philosophy is similar to the one of post-hoc tests, but instead of checking components, I am checking intervals of the domain of the functional datum



TODO
* Domanda anno scorso 
* fda regression
* functional ANOVA (classes) - regression
* independence test
* fdahotelling for the test statistic
