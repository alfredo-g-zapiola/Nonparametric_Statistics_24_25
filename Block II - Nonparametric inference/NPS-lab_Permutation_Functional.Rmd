---
title: "Lab 05 - Permutation tests for Functional Data"
date: 2024-10-25
author: "Nonparametric statistics AY 2024-2025"
output:
  
output:
  html_document: 
    df_print: paged
    toc: true
  pdf_document: default
  word_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(rgl)
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_hooks$set(webgl = hook_webgl)
```

```{css, echo=FALSE}
.extracode {
background-color: lightblue;
}
```

# Global Testing

The Objective of this lab is to show you some applications of the advanced techniques you have seen during the course, applied to functional data. Namely we will see permutation testing (both global and local...).

Let's load the packages we need and our data:

```{r}
library(fda)
library(roahd)

data=growth #data from the berkeley growth study...
```

And let's plot my curves...

```{r}
matplot(data$age,data$hgtm, type='l',col='blue')
matlines(data$age,data$hgtf, type='l',col='red')
```


What if I want to test if the two curves are equal or not? Nothing simpler.. I just need to remember how permutation tests work...

```{r}
seed=2781991
B=1000
berkeley=rbind(t(data$hgtm),t(data$hgtf))
n=nrow(berkeley)
n_m=nrow(t(data$hgtm))
n_f=nrow(t(data$hgtf))


meandiff=(colMeans(t(data$hgtm))-colMeans(t(data$hgtf)))
plot(meandiff,type = 'l')
T0=sum(meandiff^2)
T0
```

And, Knowing that under $H_0$ the two groups of curves are IID, my likelihood-invariant permutation scheme is of course label permutation, so...

```{r}
T0_perm=numeric(B)

for(perm in 1:B){
  permutation <- sample(n)
  berkeley_perm=berkeley[permutation,]
  perm_m = berkeley_perm[1:n_m,] 
  perm_f = berkeley_perm[(n_m+1):n,] 
  T0_perm[perm]=sum(((colMeans(perm_m)-colMeans(perm_f)))^2)
}

sum(T0_perm >= T0)/B
hist(T0_perm,xlim = c(0,2000))
abline(v=T0,col='green')

```

What would have happened instead, if I were to test inside a group?

```{r}
male1=berkeley[1:(n_m/2),]
male2=berkeley[(n_m/2):n_m,]
ber_m=rbind(male1,male2)

T0=sum(((colMeans(male1)-colMeans(male2)))^2)
T0

T0_perm=numeric(B)

for(perm in 1:B){
  permutation <- sample(n_m)
  berkeley_perm=ber_m[permutation,]
  perm_m = berkeley_perm[1:(n_m/2),] 
  perm_f = berkeley_perm[(n_m/2):n_m,] 
  T0_perm[perm]=sum(((colMeans(perm_m)-colMeans(perm_f)))^2)
}

sum(T0_perm >= T0)/B
hist(T0_perm)
abline(v=T0,col='green')
```

Expectedly, I am not rejecting the null hypothesis (Pvalue of the test is very high...)


Of course, I can think about using different test statistics
To do so, though, I will need a slightly different technique to treat functional data, using the package roahd.



```{r}
hgtm_fd=fData(data$age,t(data$hgtm))
hgtf_fd=fData(data$age,t(data$hgtf))

meandiff=median_fData(hgtm_fd,type='MBD')-median_fData(hgtf_fd,type='MBD')
plot(meandiff,type = 'l')
T0=(sum(abs(meandiff$values)))
T0
```
And now, the test

```{r}
berkeley_fd=append_fData(hgtm_fd,hgtf_fd)

for(perm in 1:B){
  permutation <- sample(n)
  berkeley_perm=berkeley_fd[permutation,]
  perm_m = berkeley_perm[1:n_m,] 
  perm_f = berkeley_perm[(n_m+1):n,] 
  meandiff=median_fData(perm_m,type='MBD')-median_fData(perm_f,type='MBD')
  T0_perm[perm]=sum(abs(meandiff$values))
}

sum(T0_perm >= T0)/B
hist(T0_perm,xlim = c(0,300))
abline(v=T0,col='green')
```


## Independence test

```{r}

bivar.func.data <- as.mfData(list(mfD_healthy$fDList[[1]], mfD_healthy$fDList[[2]]))
plot(bivar.func.data)

```


```{r}
#cor_spearman(bivariate_data, ordering='MHI')

MHI_first_lead <- MHI(bivariate_data$fDList[[1]]) # modified hypograph
MHI_second_lead <- MHI(bivariate_data$fDList[[2]])
cor(MHI_first_lead, MHI_second_lead)

```


## An exercise: Functional Mann-Whitney

```{r}
R <- rank.data()

R1 <- sum(R[])
R2 <- sum(R[])

U1 <- R1 - n1*(n1+1)/2  # Nr of wins of the 1st sample
U2 <- R2 - n2*(n2+1)/2  # Nr of wins of the 2nd sample


set.seed(24021979)
B <- 100000
U1.sim <- numeric(B)
U2.sim <- numeric(B)
for (k in 1:B)
{
  ranks.temp <- sample(1:n)
  R1.temp <- sum(ranks.temp[1:n1])
  R2.temp <- sum(ranks.temp[(n1+1):(n1+n2)])
  U1.temp <- R1.temp - n1*(n1+1)/2
  U2.temp <- R2.temp - n2*(n2+1)/2
  U1.sim[k] <- U1.temp
  U2.sim[k] <- U2.temp
}

hist(U1.sim, breaks = 50)
abline(v = c(U1, U2), col='red')
abline(v = n1*n2/2, lwd=3)

hist(U2.sim, breaks = 50)
abline(v = c(U1, U2), col='red')
abline(v = n1*n2/2, lwd=3)

U.star <- max(U1, U2)

p.value <- 2 * sum(U1.sim >= U.star)/B
p.value

```



# Local testing


Now, what I am doing here is basically is testing the hypothesis globally, I am rejecting if, for at least one time instant $t$ the two curves are statistically different. How do I tell what is that specific time instant? I use a procedure called Inteval-wise Testing.


### Permutation tests for functional data: unadjusted p-value function

Suppose now we also have a family of hypothesis tests:

$$
H_{0,s} = \mathbb{E}[X(s)] = \mu(s)\; vs. \; H_{1,s} : \mathbb{E}[X(s)] \neq \mu(s); \; s \in \mathcal{S}
$$
where again $X(s)$
By fixing $s$, we have a typical univariate permutation test for a hypothesised mean. So let us compute with the same loop all the different p-values of the different tests (one for each $s \in \mathcal{S}$), in an analogos way to what we did with the Bootstrap.
We take 
$$\mu(s) = 0.63, \forall s \in \mathcal{S}$$
And compute the point-wise test statistic:
$$
T(s) = |\hat{\mu}(s) - \mu(s) |\; \forall s \in \mathcal{S}
$$
where $\hat{\mu}(s)$ is the sample mean at $s$.
```{r}
mu.H0 <- 0.063
T0s <- abs(apply(func.data.sample, MARGIN=2, FUN=mean) - mu.H0)
```
And now we estimate the permutational distribution conditional on sample `func.data.sample` at point $s$

1. Estimate permutational distribution of the test statistic at each $s$
2. Compute p-value of every test $s \in \mathcal{S}$

```{r}
T.s.B = matrix(nrow=B, ncol=length(grid)) 
n <- dim(func.data.sample)[1]
p <- dim(func.data.sample)[2]

set.seed(SEED)
for(b in 1:B){

  # Permuted dataset (reflection-based)
  signs.perm = rbinom(n, 1, 0.5)*2 - 1
  
  func.perm = mu.H0 + (func.data.sample - mu.H0) * matrix(signs.perm, nrow=n,ncol=p,byrow=FALSE)
  
  
  T.s.B[b, ]  = apply(func.perm, MARGIN=2, FUN=function(x) abs(mean(x)-mu.H0) )
}
# compute element-wise p-values
p.vals <- sapply(1:p, function(i) sum(T.s.B[,i]>=T0s[i])/B )

```
So let us plot firstly the functional dataset with the hypothesised mean:
```{r}
matplot(t(func.data.sample), type="l", col="black")
lines(m1, col="red")
lines(apply(func.data.sample, MARGIN=2, FUN=mean), col="turquoise")
lines(rep(mu.H0, P), col="green")
```

And now the **unadjasted** p-value function:
```{r}
plot(x=grid, y=p.vals, type="l", main="Unadjasted p-value function")
```




Let's load the package

```{r}
#devtools::install_github("alessiapini/fdatest")
library(fdatest)
```

And let's run the test: I will show just a 2 sample case, very simple and straightforward...

```{r}

tst=IWT2(t(data$hgtm),t(data$hgtf))
plot(tst)

```

This technique allows you to perform a two sample t-test AND to impute a rejection of the null to some parts of the domain. (shadings represent significance values, dark grey is 1%, light is 5%).
The philosophy is similar to the one of post-hoc tests, but instead of checking components, I am checking intervals of the domain of the functional datum



TODO
* Domanda anno scorso 
* fda regression
* functional ANOVA (classes) - regression
* independence test
* fdahotelling for the test statistic
